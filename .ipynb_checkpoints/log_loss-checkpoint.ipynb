{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`log_loss` returns the mean of each log_loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[log-loss](https://stackoverflow.com/questions/49473587/why-is-my-implementations-of-the-log-loss-or-cross-entropy-not-producing-the-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2231435513142097"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.916290731874155"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.10536051565782628"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.41493159961539705"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.22314355, -1.60943791],\n",
       "       [-0.91629073, -0.51082562],\n",
       "       [-2.30258509, -0.10536052]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.22314355, -0.        ],\n",
       "       [-0.91629073, -0.        ],\n",
       "       [-0.        , -0.10536052]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy:\n",
      " 0.41493159961539705\n",
      "log loss:\n",
      " 0.41493159961539705\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "#log_loss(y_true, y_pred, eps=1e-15)\n",
    "# Find one by one\n",
    "a=log_loss([1,0], [0.8,0.2], eps=1e-15)# same as ln(0.8)\n",
    "b=log_loss([1,0], [0.4,0.6], eps=1e-15)# same as ln(0.4)\n",
    "c=log_loss([0,1], [0.1,0.9], eps=1e-15)# same as ln(0.1)\n",
    "a\n",
    "b\n",
    "c\n",
    "abc = a+b+c\n",
    "result1 = abc/3\n",
    "result1\n",
    "# ab = a+b\n",
    "# ab\n",
    "# result2 = ab/2\n",
    "# result2\n",
    "# print('Sum: {}'.format(abc))\n",
    "# print('Calculating one by one and finding mean: {:.4f}'.format(result1))\n",
    "\n",
    "def cross_entropy(predictions, targets):\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets * np.log(predictions)) / N\n",
    "    return ce\n",
    "\n",
    "targets = np.array([[1,0],[1,0],[0,1]])\n",
    "predictions = np.array([[0.8,0.2],[0.4,0.6],[0.1,0.9]])\n",
    "\n",
    "np.log(predictions)\n",
    "targets * np.log(predictions)\n",
    "\n",
    "print('cross entropy:\\n {}'.format(cross_entropy(predictions, targets)))\n",
    "print('log loss:\\n {}'.format(log_loss(targets, predictions)))\n",
    "\n",
    "# d = log_loss(arr1, arr2, eps=1e-15)\n",
    "\n",
    "#print(\"Using log_loss: {:.4f}\".format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21616187468057912"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.21616187468057912"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.21616187468057912"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from examples\n",
    "targets=[\"spam\", \"ham\", \"ham\", \"spam\"]\n",
    "predictions=[[.1, .9], [.9, .1], [.8, .2], [.35, .65]]\n",
    "log_loss(targets,predictions)\n",
    "\n",
    "# using np array spam as 0, ham as 1\n",
    "tar = np.array([[0,1],[1,0],[1,0],[0,1]])\n",
    "pred = np.array([[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
    "\n",
    "log_loss(tar,pred)\n",
    "# cross_entropy(pred,tar)\n",
    "\n",
    "# useing spam as 1 ham as 0\n",
    "target = [1,0,0,1]\n",
    "log_loss(target,pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Understanding binary cross-entropy / log loss: a visual explanation](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.2],\n",
       "       [-1.4],\n",
       "       [-0.8],\n",
       "       [ 0.2],\n",
       "       [ 0.4],\n",
       "       [ 0.8],\n",
       "       [ 1.2],\n",
       "       [ 2.2],\n",
       "       [ 2.9],\n",
       "       [ 4.6]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logr.predict_prba:\n",
      " [[0.81497245 0.18502755]\n",
      " [0.66932085 0.33067915]\n",
      " [0.53045326 0.46954674]\n",
      " [0.29944246 0.70055754]\n",
      " [0.26031386 0.73968614]\n",
      " [0.19261532 0.80738468]\n",
      " [0.13920934 0.86079066]\n",
      " [0.05766064 0.94233936]\n",
      " [0.03005724 0.96994276]\n",
      " [0.00590288 0.99409712]]\n",
      "logr.predict_prba, column 1:\n",
      " [0.18502755 0.33067915 0.46954674 0.70055754 0.73968614 0.80738468\n",
      " 0.86079066 0.94233936 0.96994276 0.99409712]\n",
      "y_pred:\n",
      " [0.18502755 0.33067915 0.46954674 0.70055754 0.73968614 0.80738468\n",
      " 0.86079066 0.94233936 0.96994276 0.99409712]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-2.2, -1.4, -.8, .2, .4, .8, 1.2, 2.2, 2.9, 4.6])\n",
    "y = np.array([0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "logr = LogisticRegression(solver='lbfgs')\n",
    "x.reshape(-1, 1)\n",
    "logr.fit(x.reshape(-1, 1), y)\n",
    "print('logr.predict_prba:\\n {}'.format(logr.predict_proba(x.reshape(-1, 1))))\n",
    "print('logr.predict_prba, column 1:\\n {}'.format(logr.predict_proba(x.reshape(-1, 1))[:, 1]))\n",
    "y_pred = logr.predict_proba(x.reshape(-1, 1))[:, 1].ravel()\n",
    "print('y_pred:\\n {}'.format(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.18502755, 0.33067915, 0.46954674, 0.70055754, 0.73968614,\n",
       "       0.80738468, 0.86079066, 0.94233936, 0.96994276, 0.99409712])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [-2.2 -1.4 -0.8  0.2  0.4  0.8  1.2  2.2  2.9  4.6]\n",
      "y = [0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "p(y) = [0.19 0.33 0.47 0.7  0.74 0.81 0.86 0.94 0.97 0.99]\n",
      "Log Loss / Cross Entropy = 0.3329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x = np.array([-2.2, -1.4, -.8, .2, .4, .8, 1.2, 2.2, 2.9, 4.6])\n",
    "y = np.array([0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "logr = LogisticRegression(solver='lbfgs')\n",
    "logr.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "y_pred = logr.predict_proba(x.reshape(-1, 1))[:, 1].ravel()\n",
    "y_pred\n",
    "loss = log_loss(y, y_pred)\n",
    "\n",
    "print('x = {}'.format(x))\n",
    "print('y = {}'.format(y))\n",
    "print('p(y) = {}'.format(np.round(y_pred, 2)))\n",
    "print('Log Loss / Cross Entropy = {:.4f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2231435513142097"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.916290731874155"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.10536051565782628"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logloss(true_label, predicted, eps=1e-15):\n",
    "  p = np.clip(predicted, eps, 1 - eps)\n",
    "  if true_label == 1:\n",
    "    return -math.log(p)\n",
    "  else:\n",
    "    return -math.log(1 - p)\n",
    "\n",
    "logloss(1, 0.8) # same as ln(0.8)\n",
    "logloss(1,0.4) # same as ln(0.4)\n",
    "logloss(0, 0.1) # same as ln(0.1)\n",
    "logloss(0,0.9)\n",
    "logloss(1,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7083767843022996"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7083767843022996"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def cross_entropy(predictions, targets):\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets * np.log(predictions)) / N\n",
    "    return ce\n",
    "\n",
    "predictions = np.array([[0.25,0.25,0.25,0.25],\n",
    "                        [0.01,0.01,0.01,0.97]])\n",
    "targets = np.array([[1,0,0,0],\n",
    "                   [0,0,0,1]])\n",
    "\n",
    "cross_entropy(predictions, targets)\n",
    "# 0.7083767843022996\n",
    "\n",
    "log_loss(targets, predictions)\n",
    "# 0.7083767843022996\n",
    "\n",
    "log_loss(targets, predictions) == cross_entropy(predictions, targets)\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6108604879161034"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.6094379124341003"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.array([[0.8,0.2],[0.4,0.6],[0.1,0.9]])\n",
    "targets = np.array([[1],[0],[0]])\n",
    "\n",
    "cross_entropy(predictions, targets)\n",
    "\n",
    "log_loss(targets, predictions)\n",
    "\n",
    "log_loss(targets, predictions) == cross_entropy(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
